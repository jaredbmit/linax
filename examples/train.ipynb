{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d44e7900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import torch\n",
    "import torchvision\n",
    "from jaxtyping import Array, Float, Int, PRNGKeyArray, PyTree\n",
    "\n",
    "from linax.architecture.base import AbstractModel\n",
    "from linax.architecture.linoss import LinossModel, LinossModelConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa3a56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 300\n",
    "PRINT_EVERY = 1\n",
    "SEED = 5678\n",
    "\n",
    "key = jax.random.PRNGKey(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b3a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with https://edwin-de-jong.github.io/blog/mnist-sequence-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ae9a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise_data = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "        torchvision.transforms.Lambda(lambda x: x.view(-1, 1)),\n",
    "    ]\n",
    ")\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=normalise_data,\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=normalise_data,\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ca95af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784, 1)\n",
      "(10,)\n",
      "[0 7 0 9 5 3 5 0 2 1]\n"
     ]
    }
   ],
   "source": [
    "# Checking our data a bit (by now, everyone knows what the MNIST dataset looks like)\n",
    "dummy_x, dummy_y = next(iter(trainloader))\n",
    "dummy_x = dummy_x.numpy()\n",
    "dummy_y = dummy_y.numpy()\n",
    "print(dummy_x.shape)  # 64x784x1\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f313d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key, 2)\n",
    "model = LinossModel(\n",
    "    in_features=1,\n",
    "    key=subkey,\n",
    "    out_features=10,\n",
    "    cfg=LinossModelConfig(\n",
    "        hidden_dim=16,\n",
    "    ),\n",
    ")\n",
    "state = eqx.nn.State(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75d00214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinossModel(\n",
      "  linear_encoder=Linear(\n",
      "    weight=f32[16,1], bias=None, in_features=1, out_features=16, use_bias=False\n",
      "  ),\n",
      "  linear_decoder=Linear(\n",
      "    weight=f32[10,16],\n",
      "    bias=None,\n",
      "    in_features=16,\n",
      "    out_features=10,\n",
      "    use_bias=False\n",
      "  ),\n",
      "  blocks=[\n",
      "    LinossEncoderBlock(\n",
      "      norm=BatchNorm(\n",
      "        weight=None,\n",
      "        bias=None,\n",
      "        ema_first_time_index=StateIndex(\n",
      "          marker=<object object at 0x1357dfe70>, init=bool[]\n",
      "        ),\n",
      "        ema_state_index=StateIndex(\n",
      "          marker=<object object at 0x1357dfeb0>, init=(f32[16], f32[16])\n",
      "        ),\n",
      "        batch_counter=None,\n",
      "        batch_state_index=None,\n",
      "        axis_name='batch',\n",
      "        inference=False,\n",
      "        input_size=16,\n",
      "        eps=1e-05,\n",
      "        channelwise_affine=False,\n",
      "        momentum=0.99,\n",
      "        mode='ema'\n",
      "      ),\n",
      "      sequence_mixer=LinOSSSequenceMixer(\n",
      "        A_diag=f32[16],\n",
      "        G_diag=f32[16],\n",
      "        B=f32[16,16,2],\n",
      "        C=f32[16,16,2],\n",
      "        D=f32[16],\n",
      "        steps=f32[16],\n",
      "        discretization='IMEX',\n",
      "        damping=True\n",
      "      ),\n",
      "      glu=GLU(\n",
      "        w1=Linear(\n",
      "          weight=f32[16,16],\n",
      "          bias=f32[16],\n",
      "          in_features=16,\n",
      "          out_features=16,\n",
      "          use_bias=True\n",
      "        ),\n",
      "        w2=Linear(\n",
      "          weight=f32[16,16],\n",
      "          bias=f32[16],\n",
      "          in_features=16,\n",
      "          out_features=16,\n",
      "          use_bias=True\n",
      "        )\n",
      "      ),\n",
      "      drop=Dropout(p=0.1, inference=False)\n",
      "    ),\n",
      "    LinossEncoderBlock(\n",
      "      norm=BatchNorm(\n",
      "        weight=None,\n",
      "        bias=None,\n",
      "        ema_first_time_index=StateIndex(\n",
      "          marker=<object object at 0x1357dff10>, init=bool[]\n",
      "        ),\n",
      "        ema_state_index=StateIndex(\n",
      "          marker=<object object at 0x1357dfef0>, init=(f32[16], f32[16])\n",
      "        ),\n",
      "        batch_counter=None,\n",
      "        batch_state_index=None,\n",
      "        axis_name='batch',\n",
      "        inference=False,\n",
      "        input_size=16,\n",
      "        eps=1e-05,\n",
      "        channelwise_affine=False,\n",
      "        momentum=0.99,\n",
      "        mode='ema'\n",
      "      ),\n",
      "      sequence_mixer=LinOSSSequenceMixer(\n",
      "        A_diag=f32[16],\n",
      "        G_diag=f32[16],\n",
      "        B=f32[16,16,2],\n",
      "        C=f32[16,16,2],\n",
      "        D=f32[16],\n",
      "        steps=f32[16],\n",
      "        discretization='IMEX',\n",
      "        damping=True\n",
      "      ),\n",
      "      glu=GLU(\n",
      "        w1=Linear(\n",
      "          weight=f32[16,16],\n",
      "          bias=f32[16],\n",
      "          in_features=16,\n",
      "          out_features=16,\n",
      "          use_bias=True\n",
      "        ),\n",
      "        w2=Linear(\n",
      "          weight=f32[16,16],\n",
      "          bias=f32[16],\n",
      "          in_features=16,\n",
      "          out_features=16,\n",
      "          use_bias=True\n",
      "        )\n",
      "      ),\n",
      "      drop=Dropout(p=0.1, inference=False)\n",
      "    ),\n",
      "    LinossEncoderBlock(\n",
      "      norm=BatchNorm(\n",
      "        weight=None,\n",
      "        bias=None,\n",
      "        ema_first_time_index=StateIndex(\n",
      "          marker=<object object at 0x1357dff40>, init=bool[]\n",
      "        ),\n",
      "        ema_state_index=StateIndex(\n",
      "          marker=<object object at 0x1357dff30>, init=(f32[16], f32[16])\n",
      "        ),\n",
      "        batch_counter=None,\n",
      "        batch_state_index=None,\n",
      "        axis_name='batch',\n",
      "        inference=False,\n",
      "        input_size=16,\n",
      "        eps=1e-05,\n",
      "        channelwise_affine=False,\n",
      "        momentum=0.99,\n",
      "        mode='ema'\n",
      "      ),\n",
      "      sequence_mixer=LinOSSSequenceMixer(\n",
      "        A_diag=f32[16],\n",
      "        G_diag=f32[16],\n",
      "        B=f32[16,16,2],\n",
      "        C=f32[16,16,2],\n",
      "        D=f32[16],\n",
      "        steps=f32[16],\n",
      "        discretization='IMEX',\n",
      "        damping=True\n",
      "      ),\n",
      "      glu=GLU(\n",
      "        w1=Linear(\n",
      "          weight=f32[16,16],\n",
      "          bias=f32[16],\n",
      "          in_features=16,\n",
      "          out_features=16,\n",
      "          use_bias=True\n",
      "        ),\n",
      "        w2=Linear(\n",
      "          weight=f32[16,16],\n",
      "          bias=f32[16],\n",
      "          in_features=16,\n",
      "          out_features=16,\n",
      "          use_bias=True\n",
      "        )\n",
      "      ),\n",
      "      drop=Dropout(p=0.1, inference=False)\n",
      "    ),\n",
      "    LinossEncoderBlock(\n",
      "      norm=BatchNorm(\n",
      "        weight=None,\n",
      "        bias=None,\n",
      "        ema_first_time_index=StateIndex(\n",
      "          marker=<object object at 0x1357dff20>, init=bool[]\n",
      "        ),\n",
      "        ema_state_index=StateIndex(\n",
      "          marker=<object object at 0x1357dff70>, init=(f32[16], f32[16])\n",
      "        ),\n",
      "        batch_counter=None,\n",
      "        batch_state_index=None,\n",
      "        axis_name='batch',\n",
      "        inference=False,\n",
      "        input_size=16,\n",
      "        eps=1e-05,\n",
      "        channelwise_affine=False,\n",
      "        momentum=0.99,\n",
      "        mode='ema'\n",
      "      ),\n",
      "      sequence_mixer=LinOSSSequenceMixer(\n",
      "        A_diag=f32[16],\n",
      "        G_diag=f32[16],\n",
      "        B=f32[16,16,2],\n",
      "        C=f32[16,16,2],\n",
      "        D=f32[16],\n",
      "        steps=f32[16],\n",
      "        discretization='IMEX',\n",
      "        damping=True\n",
      "      ),\n",
      "      glu=GLU(\n",
      "        w1=Linear(\n",
      "          weight=f32[16,16],\n",
      "          bias=f32[16],\n",
      "          in_features=16,\n",
      "          out_features=16,\n",
      "          use_bias=True\n",
      "        ),\n",
      "        w2=Linear(\n",
      "          weight=f32[16,16],\n",
      "          bias=f32[16],\n",
      "          in_features=16,\n",
      "          out_features=16,\n",
      "          use_bias=True\n",
      "        )\n",
      "      ),\n",
      "      drop=Dropout(p=0.1, inference=False)\n",
      "    )\n",
      "  ],\n",
      "  hidden_dim=16,\n",
      "  classification=True\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae651655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "2.494089\n"
     ]
    }
   ],
   "source": [
    "def loss(\n",
    "    model: AbstractModel,\n",
    "    x: Float[Array, \"batch 784 1\"],\n",
    "    y: Int[Array, \" batch\"],\n",
    "    state: eqx.nn.State,\n",
    "    key: PRNGKeyArray,\n",
    ") -> Float[Array, \"\"]:\n",
    "    \"\"\"Batch over loss function.\"\"\"\n",
    "    keys = jax.random.split(key, x.shape[0])\n",
    "    pred_y, model_state = jax.vmap(\n",
    "        model,\n",
    "        axis_name=\"batch\",\n",
    "        in_axes=(0, None, 0),\n",
    "        out_axes=(0, None),\n",
    "    )(x, state, keys)\n",
    "    return cross_entropy(y, pred_y), model_state\n",
    "\n",
    "\n",
    "def cross_entropy(y: Int[Array, \" batch\"], pred_y: Float[Array, \"batch 10\"]) -> Float[Array, \"\"]:\n",
    "    \"\"\"Cross entropy loss function.\"\"\"\n",
    "    pred_y = jnp.take_along_axis(pred_y, jnp.expand_dims(y, 1), axis=1)\n",
    "    return -jnp.mean(pred_y)\n",
    "\n",
    "\n",
    "# Example loss\n",
    "loss_value, _ = loss(model, dummy_x, dummy_y, state, key)\n",
    "print(loss_value.shape)  # scalar loss\n",
    "print(loss_value)  # scalar loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad28ece2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/University/thesis/linax/.venv/lib/python3.12/site-packages/jax/_src/lax/lax.py:5473: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n"
     ]
    }
   ],
   "source": [
    "(loss_value, new_state), grads = eqx.filter_value_and_grad(loss, has_aux=True)(\n",
    "    model, dummy_x, dummy_y, state, key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "613f3523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.494089\n"
     ]
    }
   ],
   "source": [
    "print(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b20042a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = eqx.filter_jit(loss)  # JIT our loss function from earlier!\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def compute_accuracy(\n",
    "    model: AbstractModel,\n",
    "    x: Float[Array, \"batch 784 1\"],\n",
    "    y: Int[Array, \" batch\"],\n",
    "    state: eqx.nn.State,\n",
    "    key: PRNGKeyArray,\n",
    ") -> Float[Array, \"\"]:\n",
    "    \"\"\"Computes the average accuracy on a batch.\"\"\"\n",
    "    keys = jax.random.split(key, x.shape[0])\n",
    "    pred_y, _ = jax.vmap(\n",
    "        model,\n",
    "        axis_name=\"batch\",\n",
    "        in_axes=(0, None, 0),\n",
    "        out_axes=(0, None),\n",
    "    )(x, state, keys)\n",
    "    pred_y = jnp.argmax(pred_y, axis=1)\n",
    "    return jnp.mean(y == pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a89df693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: AbstractModel,\n",
    "    testloader: torch.utils.data.DataLoader,\n",
    "    state: eqx.nn.State,\n",
    "    key: PRNGKeyArray,\n",
    "):\n",
    "    \"\"\"Evaluates the model on the test dataset.\"\"\"\n",
    "    inference_model = eqx.tree_inference(model, value=True)\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    for x, y in testloader:\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        # Note that all the JAX operations happen inside `loss` and `compute_accuracy`,\n",
    "        # and both have JIT wrappers, so this is fast.\n",
    "        avg_loss += loss(inference_model, x, y, state, key)[0]\n",
    "        avg_acc += compute_accuracy(inference_model, x, y, state, key)\n",
    "    return avg_loss / len(testloader), avg_acc / len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25d47041",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = optax.adamw(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9f7328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0, train_loss=2.4805493354797363, test_loss=2.401088237762451, test_accuracy=0.09579956531524658\n"
     ]
    }
   ],
   "source": [
    "def train(\n",
    "    model: AbstractModel,\n",
    "    trainloader: torch.utils.data.DataLoader,\n",
    "    testloader: torch.utils.data.DataLoader,\n",
    "    optim: optax.GradientTransformation,\n",
    "    steps: int,\n",
    "    print_every: int,\n",
    "    state: eqx.nn.State,\n",
    "    key: PRNGKeyArray,\n",
    ") -> AbstractModel:\n",
    "    \"\"\"Trains the model on the training dataset.\"\"\"\n",
    "    # Just like earlier: It only makes sense to train the arrays in our model,\n",
    "    # so filter out everything else.\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    # Always wrap everything -- computing gradients, running the optimiser, updating\n",
    "    # the model -- into a single JIT region. This ensures things run as fast as\n",
    "    # possible.\n",
    "    @eqx.filter_jit\n",
    "    def make_step(\n",
    "        model: AbstractModel,\n",
    "        opt_state: PyTree,\n",
    "        x: Float[Array, \"batch 784 1\"],\n",
    "        y: Int[Array, \" batch\"],\n",
    "        state: eqx.nn.State,\n",
    "        key: PRNGKeyArray,\n",
    "    ):\n",
    "        (loss_value, new_state), grads = eqx.filter_value_and_grad(loss, has_aux=True)(\n",
    "            model, x, y, state, key\n",
    "        )\n",
    "        updates, opt_state = optim.update(grads, opt_state, eqx.filter(model, eqx.is_array))\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss_value, new_state\n",
    "\n",
    "    # Loop over our training dataset as many times as we need.\n",
    "    def infinite_trainloader():\n",
    "        while True:\n",
    "            yield from trainloader\n",
    "\n",
    "    key, train_key = jax.random.split(key, 2)\n",
    "    for step, (x, y) in zip(range(steps), infinite_trainloader()):\n",
    "        # PyTorch dataloaders give PyTorch tensors by default,\n",
    "        # so convert them to NumPy arrays.\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        model, opt_state, train_loss, new_state = make_step(\n",
    "            model, opt_state, x, y, state, train_key\n",
    "        )\n",
    "        if (step % print_every) == 0 or (step == steps - 1):\n",
    "            test_loss, test_accuracy = evaluate(model, testloader, new_state, key)\n",
    "            print(\n",
    "                f\"{step=}, train_loss={train_loss.item()}, \"\n",
    "                f\"test_loss={test_loss.item()}, test_accuracy={test_accuracy.item()}\"\n",
    "            )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = train(model, trainloader, testloader, optim, STEPS, PRINT_EVERY, state, key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
